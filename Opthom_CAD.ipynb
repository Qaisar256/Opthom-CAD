{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yqTbwHzXtMKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ6aQja5s4DL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def load_and_preprocess_data(data_path, img_size):\n",
        "    images = []\n",
        "    labels = []\n",
        "    classes = os.listdir(data_path)\n",
        "    class_map = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(data_path, cls)\n",
        "        for img_file in os.listdir(cls_path):\n",
        "            img = cv2.imread(os.path.join(cls_path, img_file))\n",
        "            img = cv2.resize(img, img_size)\n",
        "            images.append(img)\n",
        "            labels.append(class_map[cls])\n",
        "\n",
        "    images = np.array(images, dtype='float32') / 255.0\n",
        "    labels = np.array(labels)\n",
        "    return images, labels, class_map\n",
        "\n",
        "def augment_data(images):\n",
        "    augmented_images = []\n",
        "    for img in images:\n",
        "        augmented_images.append(cv2.flip(img, 1))  # Horizontal flip\n",
        "        augmented_images.append(cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE))  # Rotate 90\n",
        "        augmented_images.append(cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE))  # Rotate -90\n",
        "    return np.array(augmented_images, dtype='float32')\n",
        "\n",
        "# Step 2: Define Swin Transformer Blocks\n",
        "class SwinTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dynamic_factor=1.0, **kwargs):\n",
        "        super(SwinTransformerBlock, self).__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ln1 = LayerNormalization()\n",
        "        self.ln2 = LayerNormalization()\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='gelu'),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        self.dynamic_factor = dynamic_factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Dynamic adjustment of input weights\n",
        "        dynamic_weights = self.dynamic_factor * tf.reduce_mean(inputs, axis=1, keepdims=True)\n",
        "        inputs = inputs + dynamic_weights\n",
        "\n",
        "        attn_output = self.mha(inputs, inputs)\n",
        "        out1 = self.ln1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.ln2(out1 + ffn_output)\n",
        "\n",
        "def build_swin_transformer(input_shape, embed_dim, num_heads, ff_dim, num_blocks, dynamic_factor=1.0):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_blocks):\n",
        "        x = SwinTransformerBlock(embed_dim, num_heads, ff_dim, dynamic_factor=dynamic_factor)(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Step 3: Classification using LightGBM\n",
        "def train_lightgbm(features, labels):\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(labels)),\n",
        "        'learning_rate': 0.1,\n",
        "        'max_depth': 8,\n",
        "        'num_leaves': 31,\n",
        "        'metric': 'multi_logloss'\n",
        "    }\n",
        "    dataset = lgb.Dataset(features, label=labels)\n",
        "    model = lgb.train(params, dataset)\n",
        "    return model\n",
        "\n",
        "# Step 4: Evaluation\n",
        "def evaluate_model(model, test_features, test_labels):\n",
        "    preds = model.predict(test_features)\n",
        "    preds_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "    acc = accuracy_score(test_labels, preds_labels)\n",
        "    precision = precision_score(test_labels, preds_labels, average='weighted')\n",
        "    recall = recall_score(test_labels, preds_labels, average='weighted')\n",
        "    f1 = f1_score(test_labels, preds_labels, average='weighted')\n",
        "\n",
        "    return acc, precision, recall, f1\n",
        "\n",
        "# Step 5: Grad-CAM for Interpretability\n",
        "def grad_cam(model, img, layer_name):\n",
        "    grad_model = Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(np.expand_dims(img, axis=0))\n",
        "        class_idx = np.argmax(predictions[0])\n",
        "        loss = predictions[:, class_idx]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)[0]\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
        "    cam = np.dot(conv_outputs[0], weights)\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    heatmap = np.uint8(255 * cam)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    return heatmap\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    data_path = 'content/drive/mydrive/optomdb/'\n",
        "    img_size = (224, 224)\n",
        "    embed_dim = 64\n",
        "    num_heads = 4\n",
        "    ff_dim = 128\n",
        "    num_blocks = 4\n",
        "    dynamic_factor = 1.2\n",
        "\n",
        "    images, labels, class_map = load_and_preprocess_data(data_path, img_size)\n",
        "    images = augment_data(images)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    swin_model = build_swin_transformer((224, 224, 3), embed_dim, num_heads, ff_dim, num_blocks, dynamic_factor=dynamic_factor)\n",
        "    train_features = swin_model.predict(X_train)\n",
        "    test_features = swin_model.predict(X_test)\n",
        "\n",
        "    lgb_model = train_lightgbm(train_features, y_train)\n",
        "    acc, precision, recall, f1 = evaluate_model(lgb_model, test_features, y_test)\n",
        "\n",
        "    print(f'Accuracy: {acc}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}')\n",
        "\n",
        "    sample_image = X_test[0]\n",
        "    heatmap = grad_cam(swin_model, sample_image, layer_name='global_average_pooling2d')\n",
        "    plt.imshow(cv2.addWeighted(sample_image, 0.6, heatmap, 0.4, 0))\n",
        "    plt.show()\n"
      ]
    }
  ]
}